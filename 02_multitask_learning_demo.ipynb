{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've expanded the previous implementation to support multi-task learning with the following key additions and modifications:\n",
    "\n",
    "1. Task-Specific Heads:\n",
    "   - SentenceClassificationHead: For sentence-level classification (e.g., sentiment analysis)\n",
    "   - NERHead: For token-level named entity recognition\n",
    "   \n",
    "2. Architecture Changes:\n",
    "   - Modified the base transformer to support both sequence-level and token-level tasks\n",
    "   - Added task-specific pooling and classification layers\n",
    "   - Implemented a unified forward pass that handles both tasks\n",
    "\n",
    "3. Multi-Task Components:\n",
    "   - Task-specific loss functions\n",
    "   - Support for different output formats per task\n",
    "   - Handling of padding for variable-length sequences\n",
    "   - Task-specific processing of transformer outputs\n",
    "\n",
    "4. Features:\n",
    "   - Classification task outputs single prediction per sentence\n",
    "   - NER task outputs predictions for each token\n",
    "   - Shared transformer backbone between tasks\n",
    "   - Task-specific loss calculations\n",
    "   - Support for padding masks and attention masks\n",
    "\n",
    "The model can be easily extended to support additional tasks by:\n",
    "1. Adding new task-specific heads\n",
    "2. Extending the forward pass to handle the new task\n",
    "3. Adding corresponding loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from multitask_learning import (\n",
    "    MultitaskTransformer,\n",
    "    MultitaskLoss,\n",
    "    create_padding_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4096\n",
    "num_classes = 3  # e.g., [positive, negative, neutral]\n",
    "num_ner_tags = 5  # e.g., [O, B-PER, I-PER, B-ORG, I-ORG]\n",
    "model = MultitaskTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    num_ner_tags=num_ner_tags\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases\n",
    "---\n",
    "**Test case 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Classification] Logits shape: torch.Size([2, 3])\n",
      "[NER] Logits shape: torch.Size([2, 10, 5])\n",
      "✔ Basic forward pass and shape tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Create sample input\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "src = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "\n",
    "# Create a padding mask (assuming pad_idx=0)\n",
    "padding_mask = create_padding_mask(src, pad_idx=0)\n",
    "\n",
    "# 1. Test Classification Task\n",
    "classification_outputs = model(src, task='classification', src_padding_mask=padding_mask)\n",
    "print(\"[Classification] Logits shape:\", classification_outputs['logits'].shape)\n",
    "assert classification_outputs['logits'].shape == (batch_size, num_classes), \\\n",
    "    \"Classification logits shape is incorrect!\"\n",
    "\n",
    "# 2. Test NER Task\n",
    "ner_outputs = model(src, task='ner', src_padding_mask=padding_mask)\n",
    "print(\"[NER] Logits shape:\", ner_outputs['logits'].shape)\n",
    "assert ner_outputs['logits'].shape == (batch_size, seq_length, num_ner_tags), \\\n",
    "    \"NER logits shape is incorrect!\"\n",
    "\n",
    "print(\"✔ Basic forward pass and shape tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test case 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Classification] Loss: 1.1429393291473389\n",
      "[NER] Loss: 1.6583757400512695\n",
      "✔ Loss computation test passed.\n"
     ]
    }
   ],
   "source": [
    "criterion = MultitaskLoss()\n",
    "\n",
    "# 1. Classification Loss\n",
    "classification_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "classification_loss = criterion(classification_outputs, classification_labels, 'classification')\n",
    "print(\"[Classification] Loss:\", classification_loss.item())\n",
    "\n",
    "# 2. NER Loss\n",
    "#   - create labels in [0, num_ner_tags-1] except for some padding tokens = -100\n",
    "ner_labels = torch.randint(0, num_ner_tags, (batch_size, seq_length))\n",
    "ner_labels[:, -2:] = -100  # artificially pad last two tokens\n",
    "ner_loss = criterion(ner_outputs, ner_labels, 'ner')\n",
    "print(\"[NER] Loss:\", ner_loss.item())\n",
    "\n",
    "assert classification_loss.dim() == 0, \"Classification loss should be a scalar!\"\n",
    "assert ner_loss.dim() == 0, \"NER loss should be a scalar!\"\n",
    "\n",
    "print(\"✔ Loss computation test passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test case 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Padding Test] Classification logits shape: torch.Size([2, 3])\n",
      "✔ Padding mask usage test passed.\n"
     ]
    }
   ],
   "source": [
    "src_with_padding = src.clone()\n",
    "src_with_padding[0, -3:] = 0  # Force some tokens to be padding in the first sequence\n",
    "padding_mask_with_padding = create_padding_mask(src_with_padding, pad_idx=0)\n",
    "\n",
    "outputs_with_padding = model(src_with_padding, task='classification', src_padding_mask=padding_mask_with_padding)\n",
    "print(\"[Padding Test] Classification logits shape:\", outputs_with_padding['logits'].shape)\n",
    "assert not torch.isnan(outputs_with_padding['logits']).any(), \"NaNs found in the output!\"\n",
    "\n",
    "print(\"✔ Padding mask usage test passed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
