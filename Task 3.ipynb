{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitask_learning import MultitaskTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4096\n",
    "num_classes = 3  # e.g., [positive, negative, neutral]\n",
    "num_ner_tags = 5  # e.g., [O, B-PER, I-PER, B-ORG, I-ORG]\n",
    "model = MultitaskTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    num_ner_tags=num_ner_tags\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Freezing the Entire Network**\n",
    "\n",
    "Implications:\n",
    "- No parameters will be updated during training\n",
    "- Useful only if the model is already well-trained for both tasks\n",
    "- Memory efficient during training\n",
    "- Fast forward pass\n",
    "- Not recommended unless the model has been pre-trained on very similar tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Freezing Only the Transformer Backbone**\n",
    "\n",
    "Implications:\n",
    "- Preserves learned language representations\n",
    "- Allows task-specific adaptation\n",
    "- Good when the backbone is pre-trained on a large corpus\n",
    "- Reduces risk of catastrophic forgetting\n",
    "- Computationally efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze transformer backbone\n",
    "for param in model.embedding.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.pos_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.transformer_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Keep task-specific heads trainable\n",
    "for param in model.classification_head.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.ner_head.parameters():\n",
    "    param.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Freezing One Task-Specific Head**\n",
    "\n",
    "Implications:\n",
    "- Useful when one task is well-optimized\n",
    "- Prevents degradation of performance on the frozen task\n",
    "- Allows fine-tuning for the other task\n",
    "- Good for incremental learning scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Freeze classification head but keep NER head trainable\n",
    "for param in model.classification_head.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Keep NER head and backbone trainable\n",
    "for param in model.ner_head.parameters():\n",
    "    param.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer Learning Approach**\n",
    "\n",
    "Rationale for Transfer Learning Choices:\n",
    "\n",
    "1. Choice of Pre-trained Model:\n",
    "    - BERT-base as starting point (proven architecture)\n",
    "    - Trained on general language understanding\n",
    "    - Well-documented transfer learning success\n",
    "    - Reasonable model size for fine-tuning\n",
    "\n",
    "2. Layer Freezing Strategy:\n",
    "    - Initial phase: Freeze everything except task heads\n",
    "    - Middle phase: Unfreeze top transformer layers\n",
    "    - Final phase: Full fine-tuning\n",
    "    - Prevents catastrophic forgetting\n",
    "    - Allows gradual adaptation\n",
    "\n",
    "3. Key Parameters:\n",
    "    - Low learning rate (2e-5) to prevent destroying pre-trained features\n",
    "    - Warmup steps to stabilize initial training\n",
    "    - Weight decay for regularization\n",
    "    - Task sampling weights based on task complexity\n",
    "\n",
    "4. Task-Specific Considerations:\n",
    "    - NER gets higher sampling weight (0.6) due to token-level complexity\n",
    "    - Classification gets lower weight (0.4) as it's sentence-level\n",
    "    - Both heads initialized randomly to learn task-specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_transfer_learning(model, base_model='bert-base-uncased'):\n",
    "    \"\"\"\n",
    "    Setup transfer learning from a pre-trained model\n",
    "    \"\"\"\n",
    "    # 1. Load pre-trained weights\n",
    "    from transformers import AutoModel\n",
    "    pretrained = AutoModel.from_pretrained(base_model)\n",
    "    \n",
    "    # 2. Copy weights for embedding and transformer layers\n",
    "    model.embedding.weight.data = pretrained.embeddings.word_embeddings.weight.data\n",
    "    \n",
    "    for i, layer in enumerate(model.transformer_encoder.layers):\n",
    "        # Copy self-attention parameters\n",
    "        layer.self_attn.in_proj_weight.data = pretrained.encoder.layer[i].attention.self.query.weight.data\n",
    "        layer.self_attn.in_proj_bias.data = pretrained.encoder.layer[i].attention.self.query.bias.data\n",
    "        \n",
    "        # Copy feedforward parameters\n",
    "        layer.linear1.weight.data = pretrained.encoder.layer[i].intermediate.dense.weight.data\n",
    "        layer.linear1.bias.data = pretrained.encoder.layer[i].intermediate.dense.bias.data\n",
    "        \n",
    "    # 3. Setup gradual unfreezing\n",
    "    layers = [\n",
    "        model.embedding,\n",
    "        model.transformer_encoder,\n",
    "        model.classification_head,\n",
    "        model.ner_head\n",
    "    ]\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def gradual_unfreeze(layers, current_epoch):\n",
    "    \"\"\"\n",
    "    Gradually unfreeze layers as training progresses\n",
    "    \"\"\"\n",
    "    if current_epoch < 2:\n",
    "        # First 2 epochs: train only task-specific heads\n",
    "        for layer in layers[:-2]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "    elif current_epoch < 4:\n",
    "        # Next 2 epochs: include last transformer layers\n",
    "        for layer in layers[1].layers[-2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    else:\n",
    "        # After 4 epochs: unfreeze all layers\n",
    "        for layer in layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    'epochs': 10,\n",
    "    'initial_lr': 2e-5,\n",
    "    'warmup_steps': 1000,\n",
    "    'weight_decay': 0.01,\n",
    "    'task_sampling_weights': {\n",
    "        'classification': 0.4,\n",
    "        'ner': 0.6\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendations:**\n",
    "\n",
    "1. For Production Use:\n",
    "    - Start with frozen backbone\n",
    "    - Gradually unfreeze layers\n",
    "    - Monitor validation performance\n",
    "    - Use early stopping per task\n",
    "\n",
    "2. For Research/Experimentation:\n",
    "    - Try different freezing combinations\n",
    "    - Experiment with layer-wise learning rates\n",
    "    - Test various pre-trained models\n",
    "    - Analyze task interference\n",
    "\n",
    "3. For Resource Constraints:\n",
    "    - Keep backbone frozen\n",
    "    - Train only task-specific heads\n",
    "    - Use smaller pre-trained models\n",
    "    - Implement gradient accumulation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
