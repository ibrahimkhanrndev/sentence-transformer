{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've implemented a sentence transformer model with several key design choices:\n",
    "\n",
    "1. Architecture Choices:\n",
    "   - Used a standard transformer encoder architecture with configurable parameters\n",
    "   - Added positional encoding to capture token position information\n",
    "   - Implemented global average pooling for creating fixed-length sentence embeddings\n",
    "   - Included support for attention masking and padding masks\n",
    "\n",
    "2. Model Components:\n",
    "   - Token Embedding Layer: Converts input tokens to dense vectors\n",
    "   - Positional Encoding: Adds position information to embeddings\n",
    "   - Transformer Encoder: Multiple layers of self-attention and feedforward networks\n",
    "   - Pooling Layer: Creates fixed-length sentence representations\n",
    "\n",
    "3. Default Hyperparameters:\n",
    "   - Embedding dimension (d_model): 512\n",
    "   - Number of attention heads: 8\n",
    "   - Number of encoder layers: 6\n",
    "   - Feedforward dimension: 2048\n",
    "   - Dropout rate: 0.1\n",
    "   - Maximum sequence length: 512\n",
    "\n",
    "4. Features:\n",
    "   - Handles variable length sequences through padding masks\n",
    "   - Scales embeddings by sqrt(d_model) as per the original transformer paper\n",
    "   - Uses adaptive average pooling for the final sentence representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformer import (\n",
    "    SentenceTransformer, \n",
    "    create_padding_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4096\n",
    "model = SentenceTransformer(vocab_size=vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases\n",
    "---\n",
    "**Test case 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10])\n",
      "Output embeddings shape: torch.Size([2, 512])\n",
      "✔ Basic shape test passed.\n"
     ]
    }
   ],
   "source": [
    "# Suppose model is already defined and instantiated as `model`\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Create random input\n",
    "src = torch.randint(0, model.embedding.num_embeddings, (batch_size, seq_len))\n",
    "\n",
    "# Create a padding mask if needed (pad_idx=0 or any index that you want as padding)\n",
    "pad_idx = 0\n",
    "padding_mask = (src == pad_idx)  # shape: [batch_size, seq_len]\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    embeddings = model(src, src_padding_mask=padding_mask)\n",
    "\n",
    "print(\"Input shape:\", src.shape)                  # e.g. [2, 10]\n",
    "print(\"Output embeddings shape:\", embeddings.shape)  # e.g. [2, 512]\n",
    "\n",
    "assert embeddings.shape == (batch_size, model.d_model), \"Output shape mismatch!\"\n",
    "print(\"✔ Basic shape test passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test case 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input with padding: tensor([[3594, 1892, 3166,  812, 2234, 1499],\n",
      "        [2131, 3148, 2565, 1655,  271, 1232],\n",
      "        [2655, 1234, 1191,    0,    0,    0]])\n",
      "Output embeddings shape: torch.Size([3, 512])\n",
      "NaN in output? False\n",
      "✔ Padding mask test passed.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "seq_len = 6\n",
    "\n",
    "# Create random input\n",
    "src = torch.randint(0, model.embedding.num_embeddings, (batch_size, seq_len))\n",
    "\n",
    "# Introduce padding in the last row from positions [3:] onward\n",
    "src[2, 3:] = 0  # artificially pad half of that sequence\n",
    "pad_idx = 0\n",
    "padding_mask = (src == pad_idx)\n",
    "\n",
    "print(\"Input with padding:\", src)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(src, src_padding_mask=padding_mask)\n",
    "\n",
    "print(\"Output embeddings shape:\", embeddings.shape)\n",
    "print(\"NaN in output?\", torch.isnan(embeddings).any().item())\n",
    "\n",
    "assert embeddings.shape == (batch_size, model.d_model), \"Output shape mismatch!\"\n",
    "assert not torch.isnan(embeddings).any(), \"Output contains NaN values!\"\n",
    "print(\"✔ Padding mask test passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test case 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1, Seq len: 5\n",
      "Embeddings shape: torch.Size([1, 512])\n",
      "\n",
      "Batch size: 4, Seq len: 8\n",
      "Embeddings shape: torch.Size([4, 512])\n",
      "\n",
      "Batch size: 2, Seq len: 16\n",
      "Embeddings shape: torch.Size([2, 512])\n",
      "\n",
      "✔ Multiple input sizes test passed.\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    (1, 5),    # (batch_size=1, seq_len=5)\n",
    "    (4, 8),    # (batch_size=4, seq_len=8)\n",
    "    (2, 16),   # (batch_size=2, seq_len=16)\n",
    "]\n",
    "\n",
    "for (batch_size, seq_len) in test_cases:\n",
    "    src = torch.randint(0, model.embedding.num_embeddings, (batch_size, seq_len))\n",
    "    pad_idx = 0\n",
    "    padding_mask = (src == pad_idx)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = model(src, src_padding_mask=padding_mask)\n",
    "    \n",
    "    print(f\"Batch size: {batch_size}, Seq len: {seq_len}\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\\n\")\n",
    "    \n",
    "    assert embeddings.shape == (batch_size, model.d_model), (\n",
    "        f\"Output shape mismatch for batch_size={batch_size}, seq_len={seq_len}\"\n",
    "    )\n",
    "\n",
    "print(\"✔ Multiple input sizes test passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test case 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw embedding shape: torch.Size([2, 10, 512])\n",
      "Positional encoding applied shape: torch.Size([2, 10, 512])\n",
      "✔ Positional encoding changes the embeddings as expected.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Get embeddings without positional encoding\n",
    "src = torch.randint(0, model.embedding.num_embeddings, (batch_size, seq_len))\n",
    "raw_emb = model.embedding(src) * (model.d_model ** 0.5)\n",
    "\n",
    "# Get embeddings from pos_encoder\n",
    "pos_emb = model.pos_encoder(raw_emb)\n",
    "\n",
    "print(\"Raw embedding shape:\", raw_emb.shape)\n",
    "print(\"Positional encoding applied shape:\", pos_emb.shape)\n",
    "\n",
    "# Check they aren't identical\n",
    "assert not torch.allclose(raw_emb, pos_emb), \"Positional encoding seems not to be applied!\"\n",
    "print(\"✔ Positional encoding changes the embeddings as expected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5836, -0.1540, -0.8739,  ..., -1.1554,  0.5629,  0.7844],\n",
       "        [ 0.2254,  1.3351,  0.0780,  ..., -0.0755, -0.9445,  0.5210]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare input tensors\n",
    "input_ids = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 6, 7, 0]])  # Padded sequences\n",
    "padding_mask = create_padding_mask(input_ids)\n",
    "\n",
    "# Get sentence embeddings\n",
    "embeddings = model(input_ids, src_padding_mask=padding_mask)\n",
    "embeddings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
